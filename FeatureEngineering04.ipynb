{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will follow other strategies for feature engineering, trying to overcome or circunvent the computational cost of the strategies in the notebooks FeatureEngineering01, 02, 03.\n",
    "First, we will try to generate a list of tokens with sentiments (filtered as FeatureEngineering01 until the step with the filter for vader sentiment, that is, excluding the calculation of our own sentiment score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load initial data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_pickle('full_data.db')\n",
    "\n",
    "#getting the object with the trained words 'NWORDS' that we saved in the previous step with 'dill'\n",
    "import dill\n",
    "with open('TrainedWordsForSpellCheck.pkl', 'rb') as f:\n",
    "    NWORDS=dill.load(f)\n",
    "    \n",
    "import re, collections\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def words(text):\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "def edits1(word):\n",
    "    s = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in s if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in s if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in s for c in alphabet if b]\n",
    "    inserts    = [a + c + b     for a, b in s for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): \n",
    "    return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or    known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)\n",
    "\n",
    "def correct_top(word, n):\n",
    "    candidates = known([word]) or known(edits1(word)) or    known_edits2(word) or [word]\n",
    "    s = sorted(candidates, key=NWORDS.get, reverse=True)\n",
    "    return s[0], s[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# building a tokenizer that picks out sequences of alphanumeric characters as tokens \n",
    "# and drops everything else \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# here we use to test a string in which we repeat several times the words 'time' and 'latin'\n",
    "dd=\"The hard hard hard harder hardest badly worse worst oldest best better loudly time classical Greek and writing had little or no space between words, and could be written in boustrophedon (alternating directions). Over time, text direction (left to right) became standardized, and word dividers and terminal punctuation became common. The first way to divide sentences into groups was the original paragraphos, similar to an underscore at the beginning of the new group.[3] The Greek paragraphos evolved into the pilcrow (Â¶), which in English manuscripts in the Middle Ages can be seen inserted inline between sentences. \"\n",
    "wordList=tokenizer.tokenize(dd)\n",
    "#spell check\n",
    "for w in wordList:\n",
    "    wordList[wordList.index(w)]=correct(w)\n",
    "#print wordList\n",
    "\n",
    "#removing stopwords\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "wordsNoStop = [w for w in wordList if not w in stopwords.words('english')]\n",
    "\n",
    "# lowercase + tokenize + spellcheck + stopwords\n",
    "def tagList(textData):\n",
    "    wordList=tokenizer.tokenize(textData.lower())\n",
    "    for w in wordList:\n",
    "        wordList[wordList.index(w)]=correct(w)\n",
    "    wordsNoStop = [w for w in wordList if not w in stopwords.words('english')]\n",
    "    tagged = pos_tag(wordsNoStop)\n",
    "    return tagged\n",
    "#tagged = tagList(dd)\n",
    "\n",
    "def subjobj(taggedList):\n",
    "#This funtion accepts a list of (word,tags) and  create two list:\n",
    "#(a) the list with subjective content (that express sentiment)...(adjectives, verbs, adverbs)\n",
    "#(b) the list with objective content (that do not express sentiment)...(nouns, pronouns)\n",
    "#-------------------------------------------------------------------------------------------\n",
    "    subjTags    = ['VB','VBP','VBZ','VBD','VBG','VBN','RB','RBR','RBS','JJ','JJR','JJS']\n",
    "    objTags     = ['NN','NNS','NNP','NNPS']\n",
    "    subjContent = []\n",
    "    objContent  = []\n",
    "    for t in taggedList:\n",
    "        if not t[0] in subjContent:\n",
    "            if t[1] in subjTags:\n",
    "                subjContent.append(t[0])\n",
    "        if not t[0] in objContent:\n",
    "            if t[1] in objTags:\n",
    "                objContent.append(t[0])\n",
    "    return subjContent, objContent \n",
    "#subjectiveList,objectiveList = subjobj(tagged)\n",
    "#print subjectiveList[:10]\n",
    "\n",
    "def lemmatizeDeduplicate(wordList):\n",
    "#This function lemmatizes the elements of the list of strings and eliminate the duplicates\n",
    "#that might result from the lemmatization returning a clean lemmatized list.\n",
    "#------------------------------------------------------------------------------------\n",
    "    #lemmatizer\n",
    "    import unicodedata\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmaTMP=[]\n",
    "    for w in wordList:\n",
    "        lemmaTMP.append(wordnet_lemmatizer.lemmatize(w))\n",
    "    lemmatizeList=[x.encode('UTF8') for x in lemmaTMP]\n",
    "    finalList=list(set(lemmatizeList)) #eliminate duplicates\n",
    "    return finalList\n",
    "#print lemmatizeDeduplicate(subjectiveList)[:10]\n",
    "\n",
    "\n",
    "def cleanList(textData):\n",
    "    wordList=tokenizer.tokenize(textData.lower())\n",
    "    for w in wordList:\n",
    "        wordList[wordList.index(w)]=correct(w)\n",
    "    cleaned = [w for w in wordList if not w in stopwords.words('english')]\n",
    "    return cleaned\n",
    "\n",
    "from vader import *\n",
    "def filterBySentiment(myList):\n",
    "# this function implements a filter in which we keep only the words in the list that\n",
    "#have some non-zero sentiment (vader scores) associated to them.\n",
    "    sentList = []\n",
    "    for x in myList:\n",
    "        if sentiment(x)[\"compound\"]<>0:\n",
    "            sentList.append( (x,sentiment(x)[\"compound\"]) )\n",
    "    return sentList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:29: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def getReducedDataSet(df,colName,nRows):\n",
    "# This function created a reduced data frame to be use in training/validation/testing\n",
    "# input :\n",
    "# df        --- the full dataset\n",
    "# colNamed  --- the name of the column (e.g., 'delivery_time','quality','service')\n",
    "# nRows     --- the number of rows per each star ([1-6]) on the output dataframe\n",
    "# this function does the following:\n",
    "# (1) creates a randomization by rows of df\n",
    "# (2) creates a 'TMP' dataframe with the relevant columns (colName + 'commentary')\n",
    "# (3) sorts 'TMP' is ascending order using 'colName'\n",
    "# (4) keeps the rows in which the stars <> 0\n",
    "# (5) created 6 smaller dataframes (d1...d6) comntaining 'nRows' rows for each star rating\n",
    "# (6) concatenates (d1...d6) into an output final dataframe\n",
    "# We can use the function to create dataframes for training with different size (larger)\n",
    "# then the dataframes for validation/testing. \n",
    "# As we are randomizing the initial dataframe at the beginning of the function we can use\n",
    "# this function to 'bootstrap' the initial dataframe and create a large number\n",
    "# of training/validation/testing sets.\n",
    "#-------------------------------------------------------------------------------\n",
    "    if colName == \"delivery_time\":\n",
    "        colNumber = 3\n",
    "    if colName == \"quality\":\n",
    "        colNumber = 4    \n",
    "    if colName == \"service\":\n",
    "        colNumber = 5    \n",
    "    TMP = df.iloc[np.random.permutation(len(df))] #randomize rows in the dataframe\n",
    "    TMP = TMP.iloc[:,[colNumber,6]] #get relevant columns\n",
    "    TMP = TMP.sort([colName], ascending=[1]) #sort data frame \n",
    "    TMP = TMP.loc[TMP[colName] <> 0] # take only ratings <> 0\n",
    "\n",
    "    #getting 'nRows' for each star (1,2,3,4,5,6)\n",
    "    d1 = TMP.loc[TMP[colName] == 1].iloc[range(nRows),:]\n",
    "    d2 = TMP.loc[TMP[colName] == 2].iloc[range(nRows),:]\n",
    "    d3 = TMP.loc[TMP[colName] == 3].iloc[range(nRows),:]\n",
    "    d4 = TMP.loc[TMP[colName] == 4].iloc[range(nRows),:]\n",
    "    d5 = TMP.loc[TMP[colName] == 5].iloc[range(nRows),:]\n",
    "    d6 = TMP.loc[TMP[colName] == 6].iloc[range(nRows),:]\n",
    "    frames = [d1, d2, d3, d4, d5, d6]\n",
    "    return pd.concat(frames)\n",
    "#example of use:\n",
    "nRows=5 # This papameter will affect the size of the Feature Vector (FV) or dictionary that will be used \n",
    "        # in the Naive Bayes classifier\n",
    "reducedDF = getReducedDataSet(df,'quality',nRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ... encode to string...---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "TMP  = reducedDF['commentary'].tolist()\n",
    "start_time = time.time()\n",
    "TMP1 = [x.encode('UTF8') for x in TMP]\n",
    "print(\"--- %s seconds ... encode to string...---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.960999965668 seconds ... clean...---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "cleaned      = map(cleanList,TMP1)\n",
    "print(\"--- %s seconds ... clean...---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.95800018311 seconds ... lemmatize...---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lemmatized   = map(lemmatizeDeduplicate,cleaned) \n",
    "print(\"--- %s seconds ... lemmatize...---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#OPTIONAL: saving/loading the lemmatized data before flatten the list of strings\n",
    "#import dill\n",
    "#with open('lemmatizedBeforeFlatten01.pkl', 'wb') as f1:\n",
    "#    dill.dump(lemmatized, f1)\n",
    "#import dill\n",
    "#with open('lemmatizedBeforeFlatten01.pkl', 'rb') as f:\n",
    "#    lemmatized=dill.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ... flat list...---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "flatList     = reduce(lambda x,y:x+y, map(list, lemmatized))\n",
    "print(\"--- %s seconds ... flat list...---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ... deduplicate...---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "deduplicated = list(set(flatList)) #eliminate duplicates\n",
    "print(\"--- %s seconds ... deduplicate...---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.39999985695 seconds ... tag...---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tagged     = pos_tag(deduplicated)\n",
    "print(\"--- %s seconds ... tag...---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ... tag...---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "SubjectiveList, ObjectiveList = subjobj(tagged)\n",
    "print(\"--- %s seconds ... tag...---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3.46700000763 seconds ... vader filter...---\n",
      "--------\n",
      "136\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "vaderFiltered = filterBySentiment(SubjectiveList)\n",
    "print(\"--- %s seconds ... vader filter...---\" % (time.time() - start_time))\n",
    "print \"--------\"\n",
    "print len(SubjectiveList)\n",
    "print len(vaderFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!!\n"
     ]
    }
   ],
   "source": [
    "#saving the final list results (tagged words, subjective tokens, objective tokens, tokens with vader sentiment)\n",
    "import dill\n",
    "with open('jeTokensWithVaderSentiments03.pkl', 'wb') as f1:\n",
    "    dill.dump(vaderFiltered, f1)\n",
    "with open('jeSubjectiveTokens03.pkl', 'wb') as f2:\n",
    "    dill.dump(SubjectiveList, f2)\n",
    "with open('jeObjectiveTokens03.pkl', 'wb') as f3:\n",
    "    dill.dump(ObjectiveList, f3)\n",
    "with open('jeTaggedTokens03.pkl', 'wb') as f4:\n",
    "    dill.dump(tagged, f4)    \n",
    "print \"done!!\"    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
