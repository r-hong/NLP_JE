{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will divide the full_data.db dataset to perform training and validation and test.\n",
    "In this case is important to take into consideration the class imbalance, that is, we have different number of reviews for \n",
    "every rating.\n",
    "Strategy:\n",
    "* For each rating type ('delivery_time','service','quality') get the number of reviews in each rating star (1 star, 2 starts, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load initial data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df=pd.read_pickle('full_data.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Functiones needed to create a clean version of the tokens in the present in a string (e.g. review)\n",
    "import dill\n",
    "with open('TrainedWordsForSpellCheck.pkl', 'rb') as f:\n",
    "    NWORDS=dill.load(f)\n",
    "\n",
    "#spell check functions\n",
    "import re, collections\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "def words(text):\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "def train(features):\n",
    "    model = collections.defaultdict(lambda: 1)\n",
    "    for f in features:\n",
    "        model[f] += 1\n",
    "    return model\n",
    "def edits1(word):\n",
    "    s = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes    = [a + b[1:] for a, b in s if b]\n",
    "    transposes = [a + b[1] + b[0] + b[2:] for a, b in s if len(b)>1]\n",
    "    replaces   = [a + c + b[1:] for a, b in s for c in alphabet if b]\n",
    "    inserts    = [a + c + b     for a, b in s for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "def known_edits2(word):\n",
    "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)\n",
    "\n",
    "def known(words): \n",
    "    return set(w for w in words if w in NWORDS)\n",
    "\n",
    "def correct(word):\n",
    "    candidates = known([word]) or known(edits1(word)) or    known_edits2(word) or [word]\n",
    "    return max(candidates, key=NWORDS.get)\n",
    "\n",
    "def correct_top(word, n):\n",
    "    candidates = known([word]) or known(edits1(word)) or    known_edits2(word) or [word]\n",
    "    s = sorted(candidates, key=NWORDS.get, reverse=True)\n",
    "    return s[0], s[:n]\n",
    "########################################\n",
    "# building a tokenizer that picks out sequences of alphanumeric characters as tokens \n",
    "# and drops everything else \n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# lowercase + tokenize + spellcheck + stopwords\n",
    "def cleanList(textData):\n",
    "    wordList=tokenizer.tokenize(textData.lower())\n",
    "    for w in wordList:\n",
    "        wordList[wordList.index(w)]=correct(w)\n",
    "    cleaned = [w for w in wordList if not w in stopwords.words('english')]\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:28: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "def getReducedDataSet(df,colName,nRows):\n",
    "# This function created a reduced data frame to be use in training/validation/testing\n",
    "# input :\n",
    "# df        --- the full dataset\n",
    "# colNamed  --- the name of the column (e.g., 'delivery_time','quality','service')\n",
    "# nRows     --- the number of rows per each star ([1-6]) on the output dataframe\n",
    "# this function does the following:\n",
    "# (1) creates a randomization by rows of df\n",
    "# (2) creates a 'TMP' dataframe with the relevant columns (colName + 'commentary')\n",
    "# (3) sorts 'TMP' is ascending order using 'colName'\n",
    "# (4) keeps the rows in which the stars <> 0\n",
    "# (5) created 6 smaller dataframes (d1...d6) comntaining 'nRows' rows for each star rating\n",
    "# (6) concatenates (d1...d6) into an output final dataframe\n",
    "# We can use the function to create dataframes for training with different size (larger)\n",
    "# then the dataframes for validation/testing. \n",
    "# As we are randomizing the initial dataframe at the beginning of the function we can use\n",
    "# this function to 'bootstrap' the initial dataframe and create a large number\n",
    "# of training/validation/testing sets.\n",
    "#-------------------------------------------------------------------------------\n",
    "    if colName == \"delivery_time\":\n",
    "        colNumber = 3\n",
    "    if colName == \"quality\":\n",
    "        colNumber = 4    \n",
    "    if colName == \"service\":\n",
    "        colNumber = 5    \n",
    "    TMP = df.iloc[np.random.permutation(len(df))] #randomize rows in the dataframe\n",
    "    TMP = TMP.iloc[:,[colNumber,6]] #get relevant columns\n",
    "    TMP = TMP.sort([colName], ascending=[1]) #sort data frame \n",
    "    TMP = TMP.loc[TMP[colName] <> 0] # take only ratings <> 0\n",
    "\n",
    "    #getting 'nRows' for each star (1,2,3,4,5,6)\n",
    "    d1 = TMP.loc[TMP[colName] == 1].iloc[range(nRows),:]\n",
    "    d2 = TMP.loc[TMP[colName] == 2].iloc[range(nRows),:]\n",
    "    d3 = TMP.loc[TMP[colName] == 3].iloc[range(nRows),:]\n",
    "    d4 = TMP.loc[TMP[colName] == 4].iloc[range(nRows),:]\n",
    "    d5 = TMP.loc[TMP[colName] == 5].iloc[range(nRows),:]\n",
    "    d6 = TMP.loc[TMP[colName] == 6].iloc[range(nRows),:]\n",
    "    frames = [d1, d2, d3, d4, d5, d6]\n",
    "    return pd.concat(frames)\n",
    "\n",
    "#function to convert a review in a binary vector (using our own list of tokens)\n",
    "#if token 'i' is present then the 'i' position of the binary vector is set 1, otherwise is set to 0\n",
    "#stablishing the type (rating) and size of ar arbitrary training set\n",
    "nRows=5\n",
    "trainingDF = getReducedDataSet(df,'quality',nRows)\n",
    "Y = trainingDF['quality'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.743999958038 seconds ... cleaning and encoding...---\n"
     ]
    }
   ],
   "source": [
    "# Cleaning reviews and creating a matrix 'cleanReviews'. cleanReviews[0] will contain a list \n",
    "# with the clean tokens from the first review in the dataframe 'trainingDF'\n",
    "import time\n",
    "start_time = time.time()\n",
    "TMP           = trainingDF['commentary'].tolist()\n",
    "TMP1          = [x.encode('UTF8') for x in TMP]\n",
    "cleanReviews  = map(cleanList,TMP1)\n",
    "print(\"--- %s seconds ... cleaning and encoding...---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#OPTIONAL: saving/loading clean reviews before binarization\n",
    "#import dill\n",
    "#with open('cleanReviewsForBinarization01.pkl', 'wb') as f1:\n",
    "#    dill.dump(cleanReviews, f1)\n",
    "#with open('cleanReviewsForBinarization01.pkl', 'rb') as f:\n",
    "#    cleanReviews=dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.018000125885 seconds ... binarizing...---\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# Binarization functions. Here we avoid using 'black box' binarization to have full control over\n",
    "# the Feature Vector we obtained by feature engineering. This step could be probably optimized later.\n",
    "# general var names\n",
    "# input:\n",
    "# - (FV)  - 'Feature Vector'        (list of tokens with sentiment)\n",
    "# - (TFR) - 'Tokens From Review'   (list of tokenize words from a review)\n",
    "# output:\n",
    "# - (BRs) - 'Binarized Reviews'     len(BRs[i]) = len(FV), if FV[0] exist in TFR then BRs[0]=1, else BRs[0]=0  \n",
    "\n",
    "#this is a test 'FV', the real one has to be obtained using feature engineering\n",
    "##FV = ['caca', 'pepe', 'like', 'place', 'sauce', 'eat', 'curry', 'get', 'money', 'offer', 'food', 'spacial', 'late', 'dry', 'late']\n",
    "import dill\n",
    "with open('jeTokensWithVaderSentiments01.pkl', 'rb') as f2:\n",
    "    tokens = dill.load(f2)\n",
    "FV = [str(tokens[i][0]) for i in range(len(tokens))]\n",
    "\n",
    "def binarizeTokenList(Val):\n",
    "    if TFR.count(Val)>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def binarizeReviews(cleanReviews,FV): \n",
    "    global TFR\n",
    "    BRs=[]\n",
    "    for i in cleanReviews:\n",
    "        TFR=i\n",
    "        #print TFR\n",
    "        BRs.append(map(binarizeTokenList,FV))\n",
    "    return np.asarray(BRs)    \n",
    "start_time = time.time()    \n",
    "binaryTrainingSet = binarizeReviews(cleanReviews,FV)\n",
    "print(\"--- %s seconds ... binarizing...---\" % (time.time() - start_time))\n",
    "print len(Y)\n",
    "print len(binaryTrainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: saving binarized data\n",
    "#import dill\n",
    "#with open('binaryTrainingSetX02.pkl', 'wb') as f1:\n",
    "#    dill.dump(binaryTrainingSet, f1)\n",
    "#with open('binaryTrainingSetY02.pkl', 'wb') as f2:\n",
    "#    dill.dump(Y, f2)\n",
    "\n",
    "#import cPickle as pickle \n",
    "#pickle.dump( Y, open( \"binaryTrainingSetY02.pkl\", \"wb\" ) )\n",
    "#pickle.dump( binaryTrainingSet, open( \"binaryTrainingSetX02.pkl\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0320000648499 seconds ... training BernoulliNB ...---\n",
      "<class 'sklearn.naive_bayes.BernoulliNB'>\n"
     ]
    }
   ],
   "source": [
    "#training the BernoulliNB classifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "start_time = time.time()\n",
    "clfNB = BernoulliNB()\n",
    "clfNB.fit(binaryTrainingSet, Y)\n",
    "print(\"--- %s seconds ... training BernoulliNB ...---\" % (time.time() - start_time))\n",
    "\n",
    "#save the classifier\n",
    "with open('BernoulliNB_classifierTMP.pkl', 'wb') as f2:\n",
    "    dill.dump(clfNB, f2)\n",
    "#vv=np.asarray([[0,1,0,0,0,0,0,1,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,1,0,0,0,0,0,0,0]])\n",
    "#print(clf.predict(vv))\n",
    "print type(clfNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#OPTIONAL: loading a saved classifier\n",
    "#import dill\n",
    "#with open('BernoulliNB_classifierTMP.pkl', 'rb') as f:\n",
    "#    bernouNB=dill.load(f)\n",
    "#print type(bernouNB)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.00699996948242 seconds ... training SVM...---\n",
      "<class 'sklearn.svm.classes.SVC'>\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: using other machine learning algorithms\n",
    "# At this point, to train another algorithm like support vector machine (SVM) in the example bellow, \n",
    "# is a relativelly simple task but other classifiers have their own complications and the theory behind them should be\n",
    "# taken into consideration.\n",
    "####################################\n",
    "# Example: Training a SVM classifier\n",
    "from sklearn import svm\n",
    "start_time = time.time()\n",
    "clfSVM = svm.SVC(decision_function_shape='ovr')\n",
    "clfSVM.fit(binaryTrainingSet, Y)\n",
    "print(\"--- %s seconds ... training SVM...---\" % (time.time() - start_time))\n",
    "print type(clfSVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The saved Bernoulli Naive Bayes classifier can now be used for predictions independently of this pipeline. A longer python script (bernouValidation.py) showcases this statement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
